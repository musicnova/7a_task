{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Новые фичи\n",
    "Цифры по mcc  \n",
    "Погода по месту  \n",
    "расстояние до дальнейшего соседа  \n",
    "максимальная продолжительность приобретений в данной точке по дням\n",
    "\n",
    "ПРОССУМИРОВАТЬ ДЕЛЬТЫ ПО РАЗНЫМ КООРДИНАТАМ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Boosters] Raiffeisen Data Cup. Baseline\n",
    "Общий подход:\n",
    "- Добавляем к каждой транзакции столбец: is_work (если транзакция находится в пределах 0.02 от дома клиента)\n",
    "- Добавляем к каждой транзакции столбец: is_home (если транзакция находится в пределах 0.02 от работы клиента)\n",
    "- Обучаем классификатор предсказывающий вероятность (is_home == 1) для транзакции\n",
    "- Обучаем классификатор предсказывающий вероятность (is_work == 1) для транзакции\n",
    "\n",
    "Точность определения местоположения:\n",
    "- для классификатора is_home: ~3x%\n",
    "- для классификатора is_work: ~2x%\n",
    "- общая оценка на Public Leaderboard: ???\n",
    "\n",
    "Примечание\n",
    "* Требуется Python версии 3.5\n",
    "* Требуется библиотека xgboost (для обучения использовалась xgboost версии 0.7.post3)\n",
    "* Требуются файлы: test_set.csv, train_set.csv в одном каталоге с данным скриптом\n",
    "* Требования к памяти: должно работать с 2Гб свободного RAM\n",
    "* Время работы: ~3 минуты (тестировалось на процессоре Intel Core i7-4770)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "MODULES_PATH = '../code/'\n",
    "if MODULES_PATH not in sys.path:\n",
    "    sys.path.append(MODULES_PATH)\n",
    "import mfuncs\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "pd.options.display.max_columns = 1000\n",
    "pd.options.display.max_colwidth = -1\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans, MeanShift, estimate_bandwidth, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import gmaps\n",
    "API_KEY = 'AIzaSyCG_RL0_kavuEaJAqEN5xXbU4h0VJUbA9M'\n",
    "gmaps.configure(api_key=API_KEY) # Your Google API key\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определим типы колонок для экономии памяти\n",
    "dtypes = {\n",
    "    'transaction_date': str,\n",
    "    'atm_address': str,\n",
    "    'country': str,\n",
    "    'city': str,\n",
    "    'amount': np.float32,\n",
    "    'currency': np.float32,\n",
    "    'mcc': str,\n",
    "    'customer_id': str,\n",
    "    'pos_address': str,\n",
    "    'atm_address': str,\n",
    "    'pos_lat': np.float32,\n",
    "    'pos_lon': np.float32,\n",
    "    'atm_lat': np.float32,\n",
    "    'atm_lon': np.float32,\n",
    "    'home_lat': np.float32,\n",
    "    'home_lon': np.float32,\n",
    "    'work_lat': np.float32,\n",
    "    'work_lon': np.float32,\n",
    "}\n",
    "df_all = pd.read_csv('../data/df_all.csv', dtype=dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обрабатываем дату транзакции и категориальные признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['currency'] = df_all['currency'].fillna(-1).astype(np.int32)\n",
    "df_all['mcc'] = df_all['mcc'].apply(lambda x: int(x.replace(',', ''))).astype(np.int32)\n",
    "df_all['city'] = df_all['city_name'].factorize()[0].astype(np.int32)\n",
    "df_all['country'] = df_all['country'].factorize()[0].astype(np.int32)\n",
    "df_all['amount'] = 10**df_all['amount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Фичи для даты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# удаляем транзакции без даты\n",
    "df_all = df_all[~df_all['transaction_date'].isnull()]\n",
    "df_all['transaction_date'] =  pd.to_datetime(df_all['transaction_date'], format='%Y-%m-%d')\n",
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['month'] = df_all.transaction_date.dt.month\n",
    "df_all['day'] = df_all.transaction_date.dt.day\n",
    "df_all['dayofyear'] = df_all.transaction_date.dt.dayofyear\n",
    "df_all['dayofweek'] = df_all.transaction_date.dt.dayofweek\n",
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# праздники\n",
    "holidays_df = pd.read_csv('../data/internal/all_holidays.csv', header=None)\n",
    "holidays_df[0] = pd.to_datetime(holidays_df[0])\n",
    "holidays_df = holidays_df[holidays_df[0].dt.year == 2017]\n",
    "holidays = holidays_df[0].dt.dayofyear.values\n",
    "df_all['is_weekend'] = (df_all.dayofweek >= 6).astype(np.int8)\n",
    "df_all['is_state_holiday'] = df_all['dayofyear'].isin(holidays).astype(np.int8)\n",
    "df_all['is_holiday'] = df_all['is_weekend'] | df_all['is_state_holiday']\n",
    "df_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Приводим адрес транзакции для pos и atm-транзакций к единообразному виду\n",
    "Просто объединяем в одну колонку и добавляем фичу - это атм или пос"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['is_atm'] = (~df_all['atm_lat'].isnull()).astype(np.int8)\n",
    "df_all['is_pos'] = (~df_all['pos_lat'].isnull()).astype(np.int8)\n",
    "\n",
    "df_all['add_lat'] = df_all['atm_lat'].fillna(0) + df_all['pos_lat'].fillna(0)\n",
    "df_all['add_lon'] = df_all['atm_lon'].fillna(0) + df_all['pos_lon'].fillna(0)\n",
    "\n",
    "df_all.drop(['atm_lat','atm_lon','pos_lat','pos_lon'], axis=1, inplace=True)\n",
    "\n",
    "df_all = df_all[~((df_all['add_lon'] == 0) & (df_all['add_lon'] == 0))]\n",
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# грязный хак, чтобы не учить КНН на новом юзере каждый раз\n",
    "df_all['fake_customer_id'] = (pd.factorize(df_all.customer_id)[0] + 1) * 100\n",
    "\n",
    "points = df_all[['fake_customer_id', 'add_lat', 'add_lon']].drop_duplicates().values\n",
    "neigh = NearestNeighbors(2, radius=100000)\n",
    "\n",
    "# расстояние до уникальных точек\n",
    "# neigh.fit(np.unique(points, axis=1))\n",
    "neigh.fit(points) \n",
    "\n",
    "distances, indices = neigh.kneighbors(df_all[['fake_customer_id', 'add_lat', 'add_lon']].values)\n",
    "df_all['distance_to_nearest_point'] = distances[:, 1]\n",
    "del df_all['fake_customer_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# кластерные фичи\n",
    "df_cluster = pd.read_csv('../data/df_cluster.csv')\n",
    "df_cluster.reset_index(drop=True, inplace=True)\n",
    "df_all.reset_index(drop=True, inplace=True)\n",
    "df_all = pd.concat([df_all, df_cluster.iloc[:, 3:]], axis=1)\n",
    "df_cluster.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерируем признаки is_home, is_work\n",
    "TODO: удалить чуваков у которых несколько домов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = df_all['home_lat'] - df_all['add_lat']\n",
    "lon = df_all['home_lon'] - df_all['add_lon']\n",
    "\n",
    "df_all['is_home'] = (np.sqrt((lat ** 2) + (lon ** 2)) <= 0.02).astype(np.int8)\n",
    "df_all['has_home'] = (~df_all['home_lon'].isnull()).astype(np.int8)\n",
    "\n",
    "lat = df_all['work_lat'] - df_all['add_lat']\n",
    "lon = df_all['work_lon'] - df_all['add_lon']\n",
    "df_all['is_work'] = (np.sqrt((lat ** 2) + (lon ** 2)) <= 0.02).astype(np.int8)\n",
    "df_all['has_work'] = (~df_all['work_lon'].isnull()).astype(np.int8)\n",
    "\n",
    "# df_all.drop(['work_lat','work_lon','home_lat','home_lon'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерируем категориальный признак для адреса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['address'] = df_all['add_lat'].apply(lambda x: \"%.02f\" % x) + ';' + df_all['add_lon'].apply(lambda x: \"%.02f\" % x)\n",
    "df_all['address'] = df_all['address'].factorize()[0].astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.sort_values(by=['customer_id', 'address', 'dayofyear'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_following_equal(arr, atype='eq'):\n",
    "    '''\n",
    "    types = eq,  eq_gr, eq_gr_unique \n",
    "    '''\n",
    "    arr = arr.values\n",
    "    val_cur = 1\n",
    "    val_max = 1\n",
    "    if atype == 'eq_gr_unique':\n",
    "        arr = np.unique(arr)\n",
    "    \n",
    "    for i in range(arr.size - 1):\n",
    "        if atype in ['eq_gr', 'eq_gr_unique']:\n",
    "            if arr[i] + 1 >= arr[i + 1]:\n",
    "                val_cur += 1\n",
    "            else:\n",
    "                val_max = max(val_cur, val_max)\n",
    "                val_cur = 1\n",
    "        else:     \n",
    "            if arr[i] == arr[i + 1]:\n",
    "                val_cur += 1\n",
    "            else:\n",
    "                val_max = max(val_cur, val_max)\n",
    "                val_cur = 1\n",
    "    return val_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# макс покупок подряд в день\n",
    "gb = df_all.groupby(['customer_id', 'address'])\n",
    "df_eq = gb['dayofyear'].apply(lambda x: get_max_following_equal(x)).reset_index()\n",
    "df_eq.rename(columns={'dayofyear': 'dayofyear_streak_inday'}, inplace=True)\n",
    "df_all = pd.merge(df_all, df_eq, on=['customer_id', 'address'], how='left')\n",
    "# макс покупок дней подряд \n",
    "gb = df_all.groupby(['customer_id', 'address'])\n",
    "df_eq = gb['dayofyear'].apply(lambda x: get_max_following_equal(x, atype='eq_gr')).reset_index()\n",
    "df_eq.rename(columns={'dayofyear': 'dayofyear_streak'}, inplace=True)\n",
    "df_all = pd.merge(df_all, df_eq, on=['customer_id', 'address'], how='left')\n",
    "# макс дней подряд\n",
    "gb = df_all.groupby(['customer_id', 'address'])\n",
    "df_eq = gb['dayofyear'].apply(lambda x: get_max_following_equal(x, atype='eq_gr_unique')).reset_index()\n",
    "df_eq.rename(columns={'dayofyear': 'dayofyear_streak_days'}, inplace=True)\n",
    "df_all = pd.merge(df_all, df_eq, on=['customer_id', 'address'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_num_closer(vals, unique=False, dist=0.02):\n",
    "    d = pairwise_distances(vals)\n",
    "    v = (d < dist).sum(axis=1)\n",
    "    if unique:\n",
    "        v -= (d == 0).sum(axis=1)\n",
    "    return pd.DataFrame(v, index=vals.index, columns=['num_neigh_dist{}_un{}'.format(dist, unique)])\n",
    "\n",
    "def get_ratio_closer(vals, unique=False, dist=0.02):\n",
    "    d = pairwise_distances(vals)\n",
    "    v = (d < dist).mean(axis=1)\n",
    "    if unique:\n",
    "        v -= (d == 0).mean(axis=1)\n",
    "    return pd.DataFrame(v, index=vals.index, columns=['ratio_neigh_dist{}_un{}'.format(dist, unique)])\n",
    "\n",
    "def get_num_far(vals, unique=False, dist=0.02):\n",
    "    d = pairwise_distances(vals)\n",
    "    v = (d >= dist).sum(axis=1)\n",
    "    return pd.DataFrame(v, index=vals.index, columns=['num_far_dist{}_un{}'.format(dist, unique)])\n",
    "\n",
    "\n",
    "def get_median_closer(vals, unique=False, dist=0.02):\n",
    "    ind = vals.index\n",
    "    vals = vals.values\n",
    "    d = pairwise_distances(vals)\n",
    "    v = (d < dist)\n",
    "    if unique:\n",
    "        v = (d<dist) & (d!=0)\n",
    "    medians = []\n",
    "    for i in range(len(vals)):\n",
    "        medians.append(np.median(vals[v[i]], axis=0))\n",
    "\n",
    "    c1 = 'median_dist{}_un{}_lat'.format(dist, unique)\n",
    "    c2 = 'median_dist{}_un{}_lon'.format(dist, unique)\n",
    "    c3 = 'median_dist{}_un{}_lat_diff'.format(dist, unique)\n",
    "    c4 = 'median_dist{}_un{}_lon_diff'.format(dist, unique)\n",
    "    c5 = 'median_dist{}_un{}_diff'.format(dist, unique)\n",
    "    df_ = pd.DataFrame(medians, index=ind, columns=[c1, c2])\n",
    "    df_[c3] = np.abs(df_[c1] - vals[:, 0])\n",
    "    df_[c4] = np.abs(df_[c2] - vals[:, 1])\n",
    "    df_[c5] = df_[c3] + df_[c4]\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['add_lat_'] = (df_all['add_lat'] * 30).astype(np.int32)\n",
    "df_all['add_lon_'] = (df_all['add_lon'] * 30).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_closer_complex(vals, unique=False, dist=0.01):\n",
    "    d = pairwise_distances(vals)\n",
    "    v = (d < dist).sum(axis=1)\n",
    "    if unique:\n",
    "        v -= (d == 0).sum(axis=1)\n",
    "    df_ = pd.DataFrame(v, index=vals.index, columns=['num_neigh_dist{}_un{}'.format(dist, unique)])\n",
    "    for dist in [0.02, 0.03]:\n",
    "        v = (d < dist).sum(axis=1)\n",
    "        if unique:\n",
    "            v -= (d == 0).sum(axis=1)\n",
    "        df_['num_neigh_dist{}_un{}'.format(dist, unique)] = v\n",
    "    for dist in [0.01, 0.02, 0.03]:\n",
    "        v = (d < dist).mean(axis=1)\n",
    "        if unique:\n",
    "            v -= (d == 0).mean(axis=1)\n",
    "        df_['ratio_neigh_dist{}_un{}'.format(dist, unique)] = v\n",
    "        \n",
    "    return df_\n",
    "\n",
    "df_clos = df_all.groupby(['add_lat_', 'add_lon_'])[['add_lat', \n",
    "                                                    'add_lon']].progress_apply(lambda x: get_num_closer_complex(x, False, 0.01))\n",
    "df_clos = df_clos.add_prefix('all_df_')\n",
    "df_all = pd.concat([df_all, df_clos], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del df_all['add_lat_']\n",
    "del df_all['add_lon_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# медианы в радиусе\n",
    "\n",
    "# df_clos = df_all.groupby('customer_id')[['add_lat', \n",
    "#                                          'add_lon']].apply(lambda x: get_median_closer(x, False, 0.01))\n",
    "# df_all = pd.concat([df_all, df_clos], axis=1)\n",
    "\n",
    "\n",
    "# df_clos = df_all.groupby('customer_id')[['add_lat', \n",
    "#                                          'add_lon']].apply(lambda x: get_median_closer(x, True, 0.01))\n",
    "# df_all = pd.concat([df_all, df_clos], axis=1)\n",
    "\n",
    "\n",
    "df_clos = df_all.groupby('customer_id')[['add_lat', \n",
    "                                         'add_lon']].apply(lambda x: get_median_closer(x, False, 0.02))\n",
    "df_all = pd.concat([df_all, df_clos], axis=1)\n",
    "\n",
    "\n",
    "df_clos = df_all.groupby('customer_id')[['add_lat', \n",
    "                                         'add_lon']].apply(lambda x: get_median_closer(x, True, 0.02))\n",
    "df_all = pd.concat([df_all, df_clos], axis=1)\n",
    "\n",
    "\n",
    "# df_clos = df_all.groupby('customer_id')[['add_lat', \n",
    "#                                          'add_lon']].apply(lambda x: get_median_closer(x, False, 0.05))\n",
    "# df_all = pd.concat([df_all, df_clos], axis=1)\n",
    "\n",
    "\n",
    "# df_clos = df_all.groupby('customer_id')[['add_lat', \n",
    "#                                          'add_lon']].apply(lambda x: get_median_closer(x, True, 0.05))\n",
    "# df_all = pd.concat([df_all, df_clos], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# кол-ва соседей за радиусом\n",
    "df_clos = df_all.groupby('customer_id')[['add_lat', 'add_lon']].apply(lambda x: get_num_far(x, False, 0.01))\n",
    "df_all = pd.concat([df_all, df_clos], axis=1)\n",
    "df_clos = df_all.groupby('customer_id')[['add_lat', 'add_lon']].apply(lambda x: get_num_far(x, False, 0.02))\n",
    "df_all = pd.concat([df_all, df_clos], axis=1)\n",
    "df_clos = df_all.groupby('customer_id')[['add_lat', 'add_lon']].apply(lambda x: get_num_far(x, False, 0.03))\n",
    "df_all = pd.concat([df_all, df_clos], axis=1)\n",
    "df_clos = df_all.groupby('customer_id')[['add_lat', 'add_lon']].apply(lambda x: get_num_far(x, False, 0.04))\n",
    "df_all = pd.concat([df_all, df_clos], axis=1)\n",
    "df_clos = df_all.groupby('customer_id')[['add_lat', 'add_lon']].apply(lambda x: get_num_far(x, False, 0.05))\n",
    "df_all = pd.concat([df_all, df_clos], axis=1)\n",
    "df_clos = df_all.groupby('customer_id')[['add_lat', 'add_lon']].apply(lambda x: get_num_far(x, False, 0.1))\n",
    "df_all = pd.concat([df_all, df_clos], axis=1)\n",
    "df_clos = df_all.groupby('customer_id')[['add_lat', 'add_lon']].apply(lambda x: get_num_far(x, False, 1))\n",
    "df_all = pd.concat([df_all, df_clos], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# кол-ва соседей внутри радиуса\n",
    "df_clos = df_all.groupby('customer_id')[['add_lat', 'add_lon']].apply(lambda x: get_num_closer(x, False, 0.01))\n",
    "df_all = pd.concat([df_all, df_clos], axis=1)\n",
    "df_clos = df_all.groupby('customer_id')[['add_lat', 'add_lon']].apply(lambda x: get_num_closer(x, True, 0.01))\n",
    "df_all = pd.concat([df_all, df_clos], axis=1)\n",
    "df_clos = df_all.groupby('customer_id')[['add_lat', 'add_lon']].apply(get_num_closer)\n",
    "df_all = pd.concat([df_all, df_clos], axis=1)\n",
    "df_clos = df_all.groupby('customer_id')[['add_lat', 'add_lon']].apply(lambda x: get_num_closer(x, True, 0.02))\n",
    "df_all = pd.concat([df_all, df_clos], axis=1)\n",
    "df_clos = df_all.groupby('customer_id')[['add_lat', 'add_lon']].apply(lambda x: get_num_closer(x, False, 0.03))\n",
    "df_all = pd.concat([df_all, df_clos], axis=1)\n",
    "df_clos = df_all.groupby('customer_id')[['add_lat', 'add_lon']].apply(lambda x: get_num_closer(x, True, 0.03))\n",
    "df_all = pd.concat([df_all, df_clos], axis=1)\n",
    "df_clos = df_all.groupby('customer_id')[['add_lat', 'add_lon']].apply(lambda x: get_num_closer(x, False, 0.04))\n",
    "df_all = pd.concat([df_all, df_clos], axis=1)\n",
    "df_clos = df_all.groupby('customer_id')[['add_lat', 'add_lon']].apply(lambda x: get_num_closer(x, True, 0.04))\n",
    "df_all = pd.concat([df_all, df_clos], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# доли соседей внутри радиуса\n",
    "df_clos = df_all.groupby('customer_id')[['add_lat', 'add_lon']].apply(lambda x: get_ratio_closer(x, False, 0.01))\n",
    "df_all = pd.concat([df_all, df_clos], axis=1)\n",
    "df_clos = df_all.groupby('customer_id')[['add_lat', 'add_lon']].apply(lambda x: get_ratio_closer(x, True, 0.01))\n",
    "df_all = pd.concat([df_all, df_clos], axis=1)\n",
    "\n",
    "\n",
    "df_clos = df_all.groupby('customer_id')[['add_lat', 'add_lon']].apply(lambda x: get_ratio_closer(x, False, 0.02))\n",
    "df_all = pd.concat([df_all, df_clos], axis=1)\n",
    "df_clos = df_all.groupby('customer_id')[['add_lat', 'add_lon']].apply(lambda x: get_ratio_closer(x, True, 0.02))\n",
    "df_all = pd.concat([df_all, df_clos], axis=1)\n",
    "\n",
    "\n",
    "df_clos = df_all.groupby('customer_id')[['add_lat', 'add_lon']].apply(lambda x: get_ratio_closer(x, False, 0.03))\n",
    "df_all = pd.concat([df_all, df_clos], axis=1)\n",
    "df_clos = df_all.groupby('customer_id')[['add_lat', 'add_lon']].apply(lambda x: get_ratio_closer(x, True, 0.03))\n",
    "df_all = pd.concat([df_all, df_clos], axis=1)\n",
    "\n",
    "\n",
    "df_clos = df_all.groupby('customer_id')[['add_lat', 'add_lon']].apply(lambda x: get_ratio_closer(x, False, 0.04))\n",
    "df_all = pd.concat([df_all, df_clos], axis=1)\n",
    "df_clos = df_all.groupby('customer_id')[['add_lat', 'add_lon']].apply(lambda x: get_ratio_closer(x, True, 0.04))\n",
    "df_all = pd.concat([df_all, df_clos], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерируем абонентские фичи отвечающие за соотношения между точками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.merge(df_all.groupby('customer_id')['amount'].count().reset_index(name='cid_trans_count'), how='left')\n",
    "df_all['cid_trans_count'] = df_all['cid_trans_count'].astype(np.int32)\n",
    "\n",
    "df_all = df_all.merge(df_all.groupby('customer_id')['amount'].agg('sum').reset_index(name='cid_trans_sum'), how='left')\n",
    "df_all['cid_trans_sum'] = df_all['cid_trans_sum'].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_count_sum_ratios(df_all, col):\n",
    "    col_count = 'cid_{}_trans_count'.format(col)\n",
    "    col_sum = 'cid_{}_trans_sum'.format(col)\n",
    "    df_ = df_all.groupby(['customer_id', col])['amount'].count().reset_index(name=col_count)\n",
    "    df_all = df_all.merge(df_, how='left')\n",
    "    df_all[col_count] = df_all[col_count].astype(np.int32)\n",
    "    df_all['ratio_{}_count'.format(col)] = df_all[col_count] / df_all['cid_trans_count']\n",
    "    \n",
    "    df_ = df_all.groupby(['customer_id', col])['amount'].agg('sum').reset_index(name=col_sum)\n",
    "    df_all = df_all.merge(df_, how='left')\n",
    "    df_all[col_sum] = df_all[col_sum].astype(np.float32)\n",
    "    df_all['ratio_{}_sum'.format(col)] = df_all[col_sum] / df_all['cid_trans_sum']\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all.city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = add_count_sum_ratios(df_all, 'address')\n",
    "df_all = add_count_sum_ratios(df_all, 'terminal_id')\n",
    "df_all = add_count_sum_ratios(df_all, 'mcc')\n",
    "df_all = add_count_sum_ratios(df_all, 'is_holiday')\n",
    "df_all = add_count_sum_ratios(df_all, 'city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in tqdm(df_all.columns):\n",
    "    if df_all[c].dtype == np.int64:\n",
    "        df_all[c] = df_all[c].astype(np.int32)\n",
    "    if df_all[c].dtype == np.float64:\n",
    "        df_all[c] = df_all[c].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['is_train'] = df_all['is_train'].astype(np.int8)\n",
    "df_all['is_atm'] = df_all['is_atm'].astype(np.int8)\n",
    "df_all['is_pos'] = df_all['is_pos'].astype(np.int8)\n",
    "df_all['has_home'] = df_all['has_home'].astype(np.int8)\n",
    "df_all['has_work'] = df_all['has_work'].astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all.dtypes.to_csv('../data/df_all_b11_dtypes.csv')\n",
    "df_all.to_csv('../data/df_all_b11.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.transaction_date.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('../data/df_all_3983.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in tqdm(df_all.columns):\n",
    "    if df_all[c].dtype == np.int64:\n",
    "        df_all[c] = df_all[c].astype(np.int32)\n",
    "    if df_all[c].dtype == np.float64:\n",
    "        df_all[c] = df_all[c].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Мои фичи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# добавим признаки после групбая\n",
    "df_gb = df_all[['customer_id','amount', 'add_lat', 'add_lon']].groupby('customer_id')\n",
    "coord_stat_df = df_gb.agg(['mean', 'max', 'min'])\n",
    "coord_stat_df['transactions_per_user'] = df_gb.agg('size')\n",
    "coord_stat_df.columns = ['_'.join(col).strip() for col in coord_stat_df.columns.values]\n",
    "coord_stat_df = coord_stat_df.astype(np.float32)\n",
    "coord_stat_df.reset_index(inplace=True)\n",
    "df_all = pd.merge(df_all, coord_stat_df, on='customer_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['add_lat', 'add_lon']\n",
    "types = ['min', 'max', 'mean']\n",
    "for c in cols:\n",
    "    for t in types:\n",
    "        df_all['{}_diff_{}'.format(c, t)] = np.abs(df_all[c] - df_all['{}_{}'.format(c, t)], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all = df_all.loc[:,~df_all.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# разности \n",
    "df_all['lat_diff_cluster_lat'] = np.abs(df_all['add_lat'] - df_all['cl_lat'], dtype=np.float32)\n",
    "df_all['lon_diff_cluster_lon'] = np.abs(df_all['add_lon'] - df_all['cl_lon'], dtype=np.float32)\n",
    "df_all['lon_diff_cluster'] = (df_all['lat_diff_cluster_lat'] + df_all['lon_diff_cluster_lon']).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Фичи mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# категории\n",
    "df_all['mcc_str'] = df_all['mcc'].astype(str).str.rjust(4, '0')\n",
    "df_mcc = pd.read_csv('../data/internal/mcc.csv')\n",
    "df_mcc = df_mcc.iloc[1:, :3]\n",
    "df_mcc.columns = ['mcc_str', 'mcc_cat1', 'mcc_cat2']\n",
    "df_mcc.drop_duplicates(subset=['mcc_str'], inplace=True)\n",
    "df_mcc['mcc_cat1'] = pd.factorize(df_mcc['mcc_cat1'])[0].astype(np.int32)\n",
    "df_mcc['mcc_cat2'] = pd.factorize(df_mcc['mcc_cat2'])[0].astype(np.int32)\n",
    "df_mcc.fillna('none', inplace=True)\n",
    "df_all = pd.merge(df_all, df_mcc, on='mcc_str', how='left')\n",
    "del df_all['mcc_str']\n",
    "df_mcc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_mcc['mcc_cat1'].fillna(-1, inplace=True)\n",
    "# df_mcc['mcc_cat2'].fillna(-1, inplace=True)\n",
    "\n",
    "# df_all = add_count_sum_ratios(df_all, 'mcc_cat1')\n",
    "# df_all = add_count_sum_ratios(df_all, 'mcc_cat2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "mos_shp = gpd.read_file('../data/internal/demography.shp')\n",
    "\n",
    "_pnts = [Point(vals.T) for vals in df_all[df_all.city_name=='Москва'][['add_lon', 'add_lat']].values]\n",
    "pnts = gpd.GeoDataFrame(geometry=_pnts)\n",
    "pnts.crs = mos_shp.crs\n",
    "\n",
    "mos_shp.drop(['NAME', 'ABBREV_AO'], 1, inplace=True)\n",
    "mos_shp['area'] = mos_shp['geometry'].area\n",
    "for c in mos_shp.columns:\n",
    "    if c not in ['geometry', 'area'] and 'index' not in c:\n",
    "        mos_shp[c + 'dens'] = mos_shp[c] / mos_shp['area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cities_with_country = gpd.sjoin(pnts, mos_shp, how=\"left\", op='intersects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = cities_with_country.drop(['geometry', 'index_right'], 1).columns\n",
    "for c in cols:\n",
    "    df_all[c] = -1\n",
    "df_all.loc[df_all.city_name=='Москва', cols] = cities_with_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# частота mcc\n",
    "df_mcc = df_all['mcc'].value_counts(normalize=True).reset_index()\n",
    "df_mcc.columns = ['mcc', 'mcc_freq']\n",
    "df_all = pd.merge(df_all, df_mcc, on='mcc', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# метро\n",
    "mos_metro = pd.read_csv('../data/internal/moscow_metro.csv')\n",
    "pet_metro = pd.read_csv('../data/internal/peter_metro.csv')\n",
    "df_metro = pd.concat([mos_metro, pet_metro])\n",
    "\n",
    "vals1 = df_all[['add_lat', 'add_lon']].values\n",
    "vals2 = df_metro[['metro_lat', 'metro_lon']].values\n",
    "X = pairwise_distances(vals1, vals2)\n",
    "dist_to_min_metro = X.min(axis=1)\n",
    "\n",
    "X[X == 0] = 10000\n",
    "df_all['dist_to_minmetro'] = X.min(axis=1)\n",
    "df_all['metro_in_01'] = (X < 0.01).sum(axis=1)\n",
    "df_all['metro_in_001'] = (X < 0.001).sum(axis=1)\n",
    "df_all['metro_in_02'] = (X < 0.02).sum(axis=1)\n",
    "df_all['metro_in_005'] = (X < 0.005).sum(axis=1)\n",
    "df_all['metro_in_03'] = (X < 0.03).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cik = pd.read_csv('../data/internal/cik_uik.csv')\n",
    "df_cik.dropna(subset=['lat_ik'], inplace=True)\n",
    "df_cik.dropna(subset=['lon_ik'], inplace=True)\n",
    "\n",
    "df_cik = df_cik[df_cik['lon_ik'] < 45]\n",
    "vals1 = df_all[['add_lat', 'add_lon']].drop_duplicates().values.astype(np.float32)\n",
    "df_vals = pd.DataFrame(vals1, columns=['add_lat', 'add_lon'])\n",
    "vals2 = df_cik[['lat_ik', 'lon_ik']].drop_duplicates().values.astype(np.float32)\n",
    "\n",
    "vals2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pairwise_distances(vals1, vals2)\n",
    "X[X == 0] = 10000\n",
    "\n",
    "df_vals['dist_to_ciktro'] = X.min(axis=1)\n",
    "df_vals['cik_in_01'] = (X < 0.01).sum(axis=1)\n",
    "df_vals['cik_in_001'] = (X < 0.001).sum(axis=1)\n",
    "df_vals['cik_in_02'] = (X < 0.02).sum(axis=1)\n",
    "df_vals['cik_in_005'] = (X < 0.005).sum(axis=1)\n",
    "df_vals['cik_in_03'] = (X < 0.03).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['add_lat_'] = np.round(df_all['add_lat'] * 10000).astype(int) \n",
    "df_all['add_lon_'] = np.round(df_all['add_lon'] * 10000).astype(int) \n",
    "df_vals['add_lat_'] = np.round(df_vals['add_lat'] * 10000).astype(int) \n",
    "df_vals['add_lon_'] = np.round(df_vals['add_lon'] * 10000).astype(int)\n",
    "del df_vals['add_lat']\n",
    "del df_vals['add_lon']\n",
    "\n",
    "df_all = pd.merge(df_all, df_vals, on=['add_lat_', 'add_lon_'], how='left')\n",
    "del X\n",
    "del df_all['add_lat_']\n",
    "del df_all['add_lon_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# погода\n",
    "# буду смотреть погоду в 18-00\n",
    "w1 = pd.read_csv('../data/internal/weather/moscow.csv', sep=';', index_col=False)\n",
    "w1['city_name'] = 'Москва'\n",
    "w1['transaction_date'] = pd.to_datetime(w1['Local time in Moscow'], format='%d.%m.%Y %H:%M')\n",
    "del w1['Local time in Moscow']\n",
    "w1 = w1[w1.transaction_date.dt.hour == 18].reset_index()\n",
    "w1['transaction_date'] = w1['transaction_date'].dt.date\n",
    "\n",
    "w2 = pd.read_csv('../data/internal/weather/peter.csv', sep=';', index_col=False)\n",
    "w2['city_name'] = 'Санкт-Петербург '\n",
    "w2['transaction_date'] = pd.to_datetime(w2['Local time in Moscow'], format='%d.%m.%Y %H:%M')\n",
    "del w2['Local time in Moscow']\n",
    "w2 = w2[w2.transaction_date.dt.hour == 18].reset_index()\n",
    "w2['transaction_date'] = w2['transaction_date'].dt.date\n",
    "\n",
    "df_weather = pd.concat([w1, w2], axis=0).reset_index()\n",
    "df_weather['transaction_date'] = pd.to_datetime(df_weather['transaction_date'])\n",
    "\n",
    "cn = df_weather['city_name'] # hardcode\n",
    "df_weather = df_weather.select_dtypes(exclude=['object'])\n",
    "df_weather['city_name'] = cn \n",
    "for c in df_weather:\n",
    "    if df_weather[c].isnull().mean() > 0.9:\n",
    "        del df_weather[c]\n",
    "# df_weather = df_weather.add_prefix('weather_')\n",
    "df_all = pd.merge(df_all, df_weather, on=['city_name', 'transaction_date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all['mcc_rm'] = df_all['mcc']\n",
    "df_all.loc[~df_all['mcc_rm'].isin(df_all['mcc_rm'].value_counts().iloc[:32].index.values), 'mcc_rm'] = 99999\n",
    "\n",
    "df_all['mcc_rm_cat1'] = df_all['mcc_cat1']\n",
    "df_all.loc[~df_all['mcc_rm_cat1'].isin(df_all['mcc_rm_cat1'].value_counts().iloc[:32].index.values), \n",
    "           'mcc_rm_cat1'] = 99999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_all, \n",
    "                    pd.get_dummies(df_all['mcc_rm'], prefix='mcc_rm_ohe').astype(np.int8)], axis=1)\n",
    "del df_all['mcc_rm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_all, \n",
    "                    pd.get_dummies(df_all['mcc_rm_cat1'], prefix='mcc_rm_cat1_ohe').astype(np.int8)], axis=1)\n",
    "del df_all['mcc_rm_cat1']\n",
    "\n",
    "df_all = pd.concat([df_all, \n",
    "                    pd.get_dummies(df_all['mcc_cat2'], prefix='mcc_cat2_ohe').astype(np.int8)], axis=1)\n",
    "del df_all['mcc_cat2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сделаем групбай какие вообще есть mcc у посетителя. Это поможет понять его привычки\n",
    "mcc_cols = [c for c in df_all.columns if 'mcc_rm_ohe' in c]\n",
    "df_mcc = df_all.groupby('customer_id')[mcc_cols].agg(['mean', 'sum'])\n",
    "df_mcc.columns = ['_'.join(col).strip() for col in df_mcc.columns.values]\n",
    "df_mcc = df_mcc.astype(np.float32)\n",
    "df_mcc = df_mcc.reset_index()\n",
    "df_mcc.head()\n",
    "df_all = pd.merge(df_all, df_mcc, on='customer_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# по объемам\n",
    "mcc_cols = [c for c in df_all.columns if 'mcc_rm_ohe' in c and 'mean' not in c and 'sum' not in c]\n",
    "mcc_cols_ = [c + '_amount' for c in mcc_cols]\n",
    "for c in mcc_cols:\n",
    "    df_all[c + '_amount'] = df_all[c] * df_all['amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mcc = df_all.groupby('customer_id')[mcc_cols_].agg(['mean', 'sum'])\n",
    "df_mcc.columns = ['_'.join(col).strip() for col in df_mcc.columns.values]\n",
    "df_mcc = df_mcc.astype(np.float32)\n",
    "df_mcc = df_mcc.reset_index()\n",
    "df_mcc.head()\n",
    "df_all = pd.merge(df_all, df_mcc, on='customer_id', how='left')\n",
    "\n",
    "# df_all['add_lat_'] = (df_all['add_lat'] * 40).astype(np.int32)\n",
    "# df_all['add_lon_'] = (df_all['add_lon'] * 40).astype(np.int32)\n",
    "\n",
    "# df_mcc = df_all.groupby(['add_lat_', 'add_lon_'])[mcc_cols_].agg(['mean', 'sum'])\n",
    "# df_mcc = df_mcc.add_suffix('_40coord')\n",
    "# df_mcc.columns = ['_'.join(col).strip() for col in df_mcc.columns.values]\n",
    "# df_mcc = df_mcc.astype(np.float32)\n",
    "# df_mcc.reset_index(inplace=True)\n",
    "# df_mcc.head()\n",
    "# df_all = pd.merge(df_all, df_mcc, on=['add_lat_', 'add_lon_'], how='left')\n",
    "\n",
    "# del df_all['add_lat_']\n",
    "# del df_all['add_lon_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# по объемам\n",
    "mcc_cols = [c for c in df_all.columns if 'mcc_rm_cat1_ohe' in c and 'mean' not in c and 'sum' not in c]\n",
    "mcc_cols_ = [c + '_amount' for c in mcc_cols]\n",
    "for c in mcc_cols:\n",
    "    df_all[c + '_amount'] = df_all[c] * df_all['amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mcc = df_all.groupby('customer_id')[mcc_cols_].agg(['mean', 'sum'])\n",
    "df_mcc.columns = ['_'.join(col).strip() for col in df_mcc.columns.values]\n",
    "df_mcc = df_mcc.astype(np.float32)\n",
    "df_mcc = df_mcc.reset_index()\n",
    "df_mcc.head()\n",
    "df_all = pd.merge(df_all, df_mcc, on='customer_id', how='left')\n",
    "\n",
    "# df_all['add_lat_'] = (df_all['add_lat'] * 40).astype(np.int32)\n",
    "# df_all['add_lon_'] = (df_all['add_lon'] * 40).astype(np.int32)\n",
    "\n",
    "# df_mcc = df_all.groupby(['add_lat_', 'add_lon_'])[mcc_cols_].agg(['mean', 'sum'])\n",
    "# df_mcc = df_mcc.add_suffix('_40coord')\n",
    "# df_mcc.columns = ['_'.join(col).strip() for col in df_mcc.columns.values]\n",
    "# df_mcc = df_mcc.astype(np.float32)\n",
    "# df_mcc.reset_index(inplace=True)\n",
    "# df_mcc.head()\n",
    "# df_all = pd.merge(df_all, df_mcc, on=['add_lat_', 'add_lon_'], how='left')\n",
    "\n",
    "# del df_all['add_lat_']\n",
    "# del df_all['add_lon_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сделаем групбай какие вообще есть mcc у посетителя. Это поможет понять его привычки\n",
    "# mcc_cols = [c for c in df_all.columns if 'mcc_cat1' in c]\n",
    "# df_mcc = df_all.groupby('customer_id')[mcc_cols].agg(['mean'])\n",
    "# df_mcc.columns = ['_'.join(col).strip() for col in df_mcc.columns.values]\n",
    "# df_mcc.reset_index(inplace=True)\n",
    "# df_mcc.head()\n",
    "# df_all = pd.merge(df_all, df_mcc, on='customer_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# сделаем групбай какие вообще есть mcc у посетителя. Это поможет понять его привычки\n",
    "mcc_cols = [c for c in df_all.columns if 'mcc_cat2_ohe' in c]\n",
    "df_mcc = df_all.groupby('customer_id')[mcc_cols].agg(['mean', 'sum'])\n",
    "df_mcc.columns = ['_'.join(col).strip() for col in df_mcc.columns.values]\n",
    "df_mcc = df_mcc.astype(np.float32)\n",
    "df_mcc.reset_index(inplace=True)\n",
    "df_mcc.head()\n",
    "df_all = pd.merge(df_all, df_mcc, on='customer_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# РАСПРЕДЕЛЕНИЕ MCC В ОКРЕСТНОСТИ ЧУВАКА\n",
    "df_all['add_lat_'] = (df_all['add_lat'] * 40).astype(np.int32)\n",
    "df_all['add_lon_'] = (df_all['add_lon'] * 40).astype(np.int32)\n",
    "\n",
    "df_mcc = df_all.groupby(['add_lat_', 'add_lon_'])[mcc_cols].agg(['mean', 'sum'])\n",
    "df_mcc = df_mcc.add_suffix('_40coord')\n",
    "df_mcc.columns = ['_'.join(col).strip() for col in df_mcc.columns.values]\n",
    "df_mcc = df_mcc.astype(np.float32)\n",
    "df_mcc.reset_index(inplace=True)\n",
    "df_mcc.head()\n",
    "df_all = pd.merge(df_all, df_mcc, on=['add_lat_', 'add_lon_'], how='left')\n",
    "\n",
    "del df_all['add_lat_']\n",
    "del df_all['add_lon_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc_cols = [c for c in df_all.columns if 'mcc_rm_ohe' in c and 'mean' not in c and 'sum' not in c]\n",
    "# РАСПРЕДЕЛЕНИЕ MCC В ОКРЕСТНОСТИ ЧУВАКА\n",
    "df_all['add_lat_'] = (df_all['add_lat'] * 100).astype(np.int32)\n",
    "df_all['add_lon_'] = (df_all['add_lon'] * 100).astype(np.int32)\n",
    "\n",
    "df_mcc = df_all.groupby(['add_lat_', 'add_lon_'])[mcc_cols].agg(['mean', 'sum'])\n",
    "df_mcc = df_mcc.add_suffix('_100coord')\n",
    "df_mcc.columns = ['_'.join(col).strip() for col in df_mcc.columns.values]\n",
    "df_mcc = df_mcc.astype(np.float32)\n",
    "df_mcc.reset_index(inplace=True)\n",
    "df_mcc.head()\n",
    "df_all = pd.merge(df_all, df_mcc, on=['add_lat_', 'add_lon_'], how='left')\n",
    "\n",
    "del df_all['add_lat_']\n",
    "del df_all['add_lon_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# РАСПРЕДЕЛЕНИЕ MCC В ОКРЕСТНОСТИ ЧУВАКА (ПРОВЕРИЛ-ЛУЧШЕ РАБОТАЕТ НА БОЛЬШИХ УЧАСТКАХ)\n",
    "df_all['add_lat_'] = (df_all['add_lat'] * 200).astype(np.int32)\n",
    "df_all['add_lon_'] = (df_all['add_lon'] * 200).astype(np.int32)\n",
    "\n",
    "df_mcc = df_all.groupby(['add_lat_', 'add_lon_'])[mcc_cols].agg(['mean', 'sum'])\n",
    "df_mcc = df_mcc.add_suffix('_200coord')\n",
    "df_mcc.columns = ['_'.join(col).strip() for col in df_mcc.columns.values]\n",
    "df_mcc = df_mcc.astype(np.float32)\n",
    "df_mcc.reset_index(inplace=True)\n",
    "df_mcc.head()\n",
    "df_all = pd.merge(df_all, df_mcc, on=['add_lat_', 'add_lon_'], how='left')\n",
    "\n",
    "del df_all['add_lat_']\n",
    "del df_all['add_lon_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# РАСПРЕДЕЛЕНИЕ MCC В ОКРЕСТНОСТИ ЧУВАКА\n",
    "# df_all['add_lat_'] = (df_all['add_lat'] * 100).astype(np.int32)\n",
    "# df_all['add_lon_'] = (df_all['add_lon'] * 100).astype(np.int32)\n",
    "\n",
    "# df_mcc = df_all.groupby(['add_lat_', 'add_lon_'])[mcc_cols].agg(['mean', 'sum'])\n",
    "# df_mcc = df_mcc.add_suffix('_100coord')\n",
    "# df_mcc.columns = ['_'.join(col).strip() for col in df_mcc.columns.values]\n",
    "# df_mcc = df_mcc.astype(np.float32)\n",
    "# df_mcc.reset_index(inplace=True)\n",
    "# df_mcc.head()\n",
    "# df_all = pd.merge(df_all, df_mcc, on=['add_lat_', 'add_lon_'], how='left')\n",
    "\n",
    "# del df_all['add_lat_']\n",
    "# del df_all['add_lon_']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Игрушки с адресами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['string'] = df_all['string'].fillna('')\n",
    "df_all['string'] = df_all['string'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all['has_street'] = df_all['string'].str.contains('улиц').astype(np.int8)\n",
    "df_all['has_pereul'] = df_all['string'].str.contains('переул').astype(np.int8)\n",
    "df_all['has_bulvar'] = df_all['string'].str.contains('бульв').astype(np.int8)\n",
    "df_all['has_prospekt'] = df_all['string'].str.contains('проспект').astype(np.int8)\n",
    "df_all['has_shosse'] = df_all['string'].str.contains('шосс').astype(np.int8)\n",
    "\n",
    "df_all['has_torg'] = df_all['string'].str.contains('торгов').astype(np.int8)\n",
    "df_all['has_bus'] = df_all['string'].str.contains('бизн').astype(np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Медианы по юзеру и по без дубликатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_med = df_all.groupby('customer_id')['add_lat', 'add_lon'].agg('median').reset_index()\n",
    "df_med.columns = ['customer_id', 'add_lat_median', 'add_lon_median']\n",
    "df_all = pd.merge(df_all, df_med, on='customer_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_med = df_all.drop_duplicates(subset=['customer_id', \n",
    "                                        'add_lat', 'add_lon']).groupby('customer_id')['add_lat', 'add_lon'].agg('median').reset_index()\n",
    "df_med.columns = ['customer_id', 'add_lat_median_unique', 'add_lon_median_unique']\n",
    "df_all = pd.merge(df_all, df_med, on='customer_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['lat_diff_median'] = np.abs(df_all['add_lat'] - df_all['add_lat_median'])\n",
    "df_all['lon_diff_median'] = np.abs(df_all['add_lon'] - df_all['add_lat_median'])\n",
    "df_all['lat_diff_median_unique'] = np.abs(df_all['add_lat'] - df_all['add_lat_median_unique'])\n",
    "df_all['lon_diff_median_unique'] = np.abs(df_all['add_lon'] - df_all['add_lon_median_unique'])\n",
    "\n",
    "df_all['diff_median'] = df_all['lat_diff_median'] + df_all['lon_diff_median']\n",
    "df_all['diff_median_unique'] = df_all['lat_diff_median_unique'] + df_all['lon_diff_median_unique']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv('../data/df_all_2lvl.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.dtypes.to_csv('../data/df_all_2lvl_dtypes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OSM\n",
    "https://wiki.openstreetmap.org/wiki/RU:%D0%9E%D0%B1%D1%8A%D0%B5%D0%BA%D1%82%D1%8B_%D0%BA%D0%B0%D1%80%D1%82%D1%8B#.D0.9A.D0.BE.D0.BC.D0.BC.D0.B5.D1.80.D1.87.D0.B5.D1.81.D0.BA.D0.B8.D0.B5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ogr\n",
    "driver=ogr.GetDriverByName('OSM')\n",
    "data = driver.Open('../data/internal/map.osm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlayer = data.GetLayerCount() # 5\n",
    "print(nlayer)\n",
    "features = []\n",
    "for i in range(nlayer):\n",
    "    features += [x for x in data.GetLayerByIndex(i)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fast_food  \n",
    "food_court  \n",
    "файзен raiffeisen  \n",
    "railway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### расстояние до бизнес центров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = []\n",
    "for f in tqdm(features):\n",
    "    s = str(f.ExportToJson(as_object=True)).lower()\n",
    "    if 'бизнес' in s and 'центр' in s:\n",
    "        el = f.ExportToJson(as_object=True)['geometry']['coordinates'][0]\n",
    "        if type(el) != float:\n",
    "            coords.append(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals1 = df_all[['add_lon', 'add_lat']].drop_duplicates().values.astype(np.float32)\n",
    "df_vals = pd.DataFrame(vals1, columns=['add_lat', 'add_lon'])\n",
    "vals2 = np.array(coords, dtype=np.float32)\n",
    "vals1.shape, vals2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pairwise_distances(vals1, vals2)\n",
    "X[X == 0] = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suf = 'bc'\n",
    "df_vals[suf + '_dist_to'] = X.min(axis=1)\n",
    "df_vals[suf + '_in_01'] = (X < 0.01).sum(axis=1)\n",
    "df_vals[suf + '_in_001'] = (X < 0.001).sum(axis=1)\n",
    "df_vals[suf + '_in_02'] = (X < 0.02).sum(axis=1)\n",
    "df_vals[suf + '_in_005'] = (X < 0.005).sum(axis=1)\n",
    "df_vals[suf + '_in_03'] = (X < 0.03).sum(axis=1)\n",
    "\n",
    "df_all['add_lat_'] = np.round(df_all['add_lat'] * 10000).astype(int) \n",
    "df_all['add_lon_'] = np.round(df_all['add_lon'] * 10000).astype(int) \n",
    "df_vals['add_lat_'] = np.round(df_vals['add_lat'] * 10000).astype(int) \n",
    "df_vals['add_lon_'] = np.round(df_vals['add_lon'] * 10000).astype(int)\n",
    "del df_vals['add_lat']\n",
    "del df_vals['add_lon']\n",
    "\n",
    "df_all = pd.merge(df_all, df_vals, on=['add_lat_', 'add_lon_'], how='left')\n",
    "del X\n",
    "del df_all['add_lat_']\n",
    "del df_all['add_lon_']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### до фастфудов\n",
    "http://andrewgaidus.com/Convert_OSM_Data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = ogr.GetDriverByName('OSM')\n",
    "data = driver.Open('../data/internal/map.osm')\n",
    "layer_p = data.GetLayer('points') # 5\n",
    "features_p = [x for x in layer_p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = []\n",
    "for f in tqdm(features_p):\n",
    "    s = str(f.ExportToJson(as_object=True)).lower()\n",
    "    if 'fast_food' in s:\n",
    "        coords.append(f.ExportToJson(as_object=True)['geometry']['coordinates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals1 = df_all[['add_lon', 'add_lat']].drop_duplicates().values.astype(np.float32)\n",
    "df_vals = pd.DataFrame(vals1, columns=['add_lat', 'add_lon'])\n",
    "vals2 = np.array(coords, dtype=np.float32)\n",
    "vals1.shape, vals2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pairwise_distances(vals1, vals2)\n",
    "X[X == 0] = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suf = 'fastfood'\n",
    "df_vals[suf + '_dist_to'] = X.min(axis=1)\n",
    "df_vals[suf + '_in_01'] = (X < 0.01).sum(axis=1)\n",
    "df_vals[suf + '_in_001'] = (X < 0.001).sum(axis=1)\n",
    "df_vals[suf + '_in_02'] = (X < 0.02).sum(axis=1)\n",
    "df_vals[suf + '_in_005'] = (X < 0.005).sum(axis=1)\n",
    "df_vals[suf + '_in_03'] = (X < 0.03).sum(axis=1)\n",
    "\n",
    "df_all['add_lat_'] = np.round(df_all['add_lat'] * 10000).astype(int) \n",
    "df_all['add_lon_'] = np.round(df_all['add_lon'] * 10000).astype(int) \n",
    "df_vals['add_lat_'] = np.round(df_vals['add_lat'] * 10000).astype(int) \n",
    "df_vals['add_lon_'] = np.round(df_vals['add_lon'] * 10000).astype(int)\n",
    "del df_vals['add_lat']\n",
    "del df_vals['add_lon']\n",
    "\n",
    "df_all = pd.merge(df_all, df_vals, on=['add_lat_', 'add_lon_'], how='left')\n",
    "del X\n",
    "del df_all['add_lat_']\n",
    "del df_all['add_lon_']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## станции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = []\n",
    "for f in tqdm(features_p):\n",
    "    s = str(f.ExportToJson(as_object=True)).lower()\n",
    "    if 'railway' in s:\n",
    "        coords.append(f.ExportToJson(as_object=True)['geometry']['coordinates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals1 = df_all[['add_lon', 'add_lat']].drop_duplicates().values.astype(np.float32)\n",
    "df_vals = pd.DataFrame(vals1, columns=['add_lat', 'add_lon'])\n",
    "vals2 = np.array(coords, dtype=np.float32)\n",
    "vals1.shape, vals2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pairwise_distances(vals1, vals2)\n",
    "X[X == 0] = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suf = 'rail'\n",
    "df_vals[suf + '_dist_to'] = X.min(axis=1)\n",
    "df_vals[suf + '_in_01'] = (X < 0.01).sum(axis=1)\n",
    "df_vals[suf + '_in_001'] = (X < 0.001).sum(axis=1)\n",
    "df_vals[suf + '_in_02'] = (X < 0.02).sum(axis=1)\n",
    "df_vals[suf + '_in_005'] = (X < 0.005).sum(axis=1)\n",
    "df_vals[suf + '_in_03'] = (X < 0.03).sum(axis=1)\n",
    "\n",
    "df_all['add_lat_'] = np.round(df_all['add_lat'] * 10000).astype(int) \n",
    "df_all['add_lon_'] = np.round(df_all['add_lon'] * 10000).astype(int) \n",
    "df_vals['add_lat_'] = np.round(df_vals['add_lat'] * 10000).astype(int) \n",
    "df_vals['add_lon_'] = np.round(df_vals['add_lon'] * 10000).astype(int)\n",
    "del df_vals['add_lat']\n",
    "del df_vals['add_lon']\n",
    "\n",
    "df_all = pd.merge(df_all, df_vals, on=['add_lat_', 'add_lon_'], how='left')\n",
    "del X\n",
    "del df_all['add_lat_']\n",
    "del df_all['add_lon_']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## райф"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = []\n",
    "for f in tqdm(features_p):\n",
    "    s = str(f.ExportToJson(as_object=True)).lower()\n",
    "    if 'райф' in s or 'raiffeisen' in s:\n",
    "        coords.append(f.ExportToJson(as_object=True)['geometry']['coordinates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals1 = df_all[['add_lon', 'add_lat']].drop_duplicates().values.astype(np.float32)\n",
    "df_vals = pd.DataFrame(vals1, columns=['add_lat', 'add_lon'])\n",
    "vals2 = np.array(coords, dtype=np.float32)\n",
    "vals1.shape, vals2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pairwise_distances(vals1, vals2)\n",
    "X[X == 0] = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suf = 'raif1'\n",
    "df_vals[suf + '_dist_to'] = X.min(axis=1)\n",
    "df_vals[suf + '_in_01'] = (X < 0.01).sum(axis=1)\n",
    "df_vals[suf + '_in_001'] = (X < 0.001).sum(axis=1)\n",
    "df_vals[suf + '_in_02'] = (X < 0.02).sum(axis=1)\n",
    "df_vals[suf + '_in_005'] = (X < 0.005).sum(axis=1)\n",
    "df_vals[suf + '_in_03'] = (X < 0.03).sum(axis=1)\n",
    "\n",
    "df_all['add_lat_'] = np.round(df_all['add_lat'] * 10000).astype(int) \n",
    "df_all['add_lon_'] = np.round(df_all['add_lon'] * 10000).astype(int) \n",
    "df_vals['add_lat_'] = np.round(df_vals['add_lat'] * 10000).astype(int) \n",
    "df_vals['add_lon_'] = np.round(df_vals['add_lon'] * 10000).astype(int)\n",
    "del df_vals['add_lat']\n",
    "del df_vals['add_lon']\n",
    "\n",
    "df_all = pd.merge(df_all, df_vals, on=['add_lat_', 'add_lon_'], how='left')\n",
    "del X\n",
    "del df_all['add_lat_']\n",
    "del df_all['add_lon_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = []\n",
    "for f in tqdm(features):\n",
    "    s = str(f.ExportToJson(as_object=True)).lower()\n",
    "    if 'райф' in s or 'raiffeisen' in s:\n",
    "        el = f.ExportToJson(as_object=True)['geometry']['coordinates'][0]\n",
    "        if type(el) != float:\n",
    "            coords.append(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals1 = df_all[['add_lon', 'add_lat']].drop_duplicates().values.astype(np.float32)\n",
    "df_vals = pd.DataFrame(vals1, columns=['add_lat', 'add_lon'])\n",
    "vals2 = np.array(coords, dtype=np.float32)\n",
    "vals1.shape, vals2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pairwise_distances(vals1, vals2)\n",
    "X[X == 0] = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suf = 'raif2'\n",
    "df_vals[suf + '_dist_to'] = X.min(axis=1)\n",
    "df_vals[suf + '_in_01'] = (X < 0.01).sum(axis=1)\n",
    "df_vals[suf + '_in_001'] = (X < 0.001).sum(axis=1)\n",
    "df_vals[suf + '_in_02'] = (X < 0.02).sum(axis=1)\n",
    "df_vals[suf + '_in_005'] = (X < 0.005).sum(axis=1)\n",
    "df_vals[suf + '_in_03'] = (X < 0.03).sum(axis=1)\n",
    "\n",
    "df_all['add_lat_'] = np.round(df_all['add_lat'] * 10000).astype(int) \n",
    "df_all['add_lon_'] = np.round(df_all['add_lon'] * 10000).astype(int) \n",
    "df_vals['add_lat_'] = np.round(df_vals['add_lat'] * 10000).astype(int) \n",
    "df_vals['add_lon_'] = np.round(df_vals['add_lon'] * 10000).astype(int)\n",
    "del df_vals['add_lat']\n",
    "del df_vals['add_lon']\n",
    "\n",
    "df_all = pd.merge(df_all, df_vals, on=['add_lat_', 'add_lon_'], how='left')\n",
    "del X\n",
    "del df_all['add_lat_']\n",
    "del df_all['add_lon_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vals2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.shape, df_all.columns.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.loc[:,~df_all.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ys = ['is_home', 'is_work']\n",
    "drop_cols = ['atm_address', 'customer_id', 'pos_address', 'terminal_id', 'transaction_date', \n",
    "             'is_home' ,'has_home', 'is_work', 'has_work', 'is_train', 'city_name']\n",
    "drop_cols += ['work_lat','work_lon','home_lat','home_lon', 'string']\n",
    "\n",
    "drop_cols += ['pred:is_home', 'pred:is_work']\n",
    "# cols = [c for c in df_all.columns if 'median_dist' in c]\n",
    "# cols = [c for c in df_all.columns if 'lat' in c or 'lon' in c and 'diff' not in c and 'median' not in c]\n",
    "# cols += ['address']\n",
    "# drop_cols += cols\n",
    "\n",
    "cols = [c for c in df_all.columns if 'mcc_ohe' in c and 'mean' not in c]\n",
    "# cols += ['address']\n",
    "drop_cols += cols\n",
    "\n",
    "\n",
    "y_cols = ['is_home', 'is_work']\n",
    "usecols = df_all.drop(drop_cols, 1, errors='ignore').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'num_leaves': 511,\n",
    "    'learning_rate': 0.01,\n",
    "    'metric' : 'binary_logloss',\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 1,\n",
    "    'num_threads': 12,\n",
    "    'verbose': 0,\n",
    "}\n",
    "\n",
    "model = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col = 'is_home'\n",
    "\n",
    "cust_train = df_all[df_all['is_train']==1].groupby('customer_id')[y_col.replace('is_','has_')].max()\n",
    "cust_train = cust_train[cust_train > 0].index\n",
    "\n",
    "cust_train, cust_valid = train_test_split(cust_train, test_size=0.2, shuffle=True, random_state=111)\n",
    "\n",
    "df_train = pd.DataFrame(cust_train, columns=['customer_id']).merge(df_all, how='left')\n",
    "df_valid = pd.DataFrame(cust_valid, columns=['customer_id']).merge(df_all, how='left')\n",
    "\n",
    "lgb_train = lgb.Dataset(df_train[usecols], df_train[y_col])\n",
    "lgb_valid = lgb.Dataset(df_valid[usecols], df_valid[y_col])\n",
    "\n",
    "gbm_h = lgb.train(params,\n",
    "                lgb_train,\n",
    "                valid_sets=[lgb_valid],\n",
    "                num_boost_round=2000,\n",
    "                verbose_eval=30,\n",
    "                early_stopping_rounds=100)\n",
    "\n",
    "model[y_col] = gbm_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col = 'is_work'\n",
    "\n",
    "cust_train = df_all[df_all['is_train']==1].groupby('customer_id')[y_col.replace('is_','has_')].max()\n",
    "cust_train = cust_train[cust_train > 0].index\n",
    "\n",
    "cust_train, cust_valid = train_test_split(cust_train, test_size=0.2, shuffle=True, random_state=111)\n",
    "\n",
    "\n",
    "\n",
    "df_train = pd.DataFrame(cust_train, columns=['customer_id']).merge(df_all, how='left')\n",
    "df_valid = pd.DataFrame(cust_valid, columns=['customer_id']).merge(df_all, how='left')\n",
    "\n",
    "lgb_train = lgb.Dataset(df_train[usecols], df_train[y_col])\n",
    "lgb_valid = lgb.Dataset(df_valid[usecols], df_valid[y_col])\n",
    "\n",
    "gbm_w = lgb.train(params,\n",
    "                lgb_train,\n",
    "                valid_sets=[lgb_valid],\n",
    "                num_boost_round=2000,\n",
    "                verbose_eval=30,\n",
    "                early_stopping_rounds=100)\n",
    "\n",
    "model[y_col] = gbm_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Полезные MCC\n",
    "дом\n",
    "6011 - финансы\n",
    "5411 - придомовые магазы\n",
    "5814 - мак\n",
    "5912 - аптеки\n",
    "5921 - пиво\n",
    "5499 - магазы пяторочка типа\n",
    "5812 - рестроанчики\n",
    "работа\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(14, 10)\n",
    "lgb.plot_importance(gbm_h, max_num_features=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _best(x):\n",
    "    ret = None\n",
    "    for col in ys:\n",
    "        pred = ('pred:%s' % col)\n",
    "        if pred in x:\n",
    "            i = (x[pred].idxmax())\n",
    "            cols = [pred, 'add_lat', 'add_lon']\n",
    "            if col in x:\n",
    "                cols.append(col)\n",
    "            tmp = x.loc[i,cols]\n",
    "            tmp.rename({\n",
    "                'add_lat':'%s:add_lat' % col,\n",
    "                'add_lon':'%s:add_lon' % col,\n",
    "            }, inplace = True)\n",
    "            if ret is None:\n",
    "                ret = tmp\n",
    "            else:\n",
    "                ret = pd.concat([ret, tmp])\n",
    "    return ret\n",
    "\n",
    "\n",
    "def predict_proba(dt, ys=['is_home', 'is_work']):\n",
    "    for col in ys:\n",
    "        pred = ('pred:%s' % col)\n",
    "        dt[pred] = model[col].predict(dt[usecols])\n",
    "    return dt.groupby('customer_id').apply(_best).reset_index()\n",
    "\n",
    "def score(dt, ys=['is_home', 'is_work'], return_df=False):\n",
    "    dt_ret = predict_proba(dt, ys)\n",
    "    if return_df:\n",
    "        return dt_ret\n",
    "    mean = 0.0\n",
    "    for col in ys:\n",
    "        col_mean = dt_ret[col].mean()\n",
    "        mean += col_mean\n",
    "    if len(ys) == 2:\n",
    "        mean = mean / len(ys)\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Train accuracy:\", score(df_train, ys=['is_home']))\n",
    "print (\"Test accuracy:\", score(df_valid, ys=['is_home']))\n",
    "\n",
    "print (\"Train accuracy:\", score(df_train, ys=['is_work']))\n",
    "print (\"Test accuracy:\", score(df_valid, ys=['is_work']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train accuracy: 0.5458070770722249\n",
    "Test accuracy: 0.5494186046511628\n",
    "Train accuracy: 0.4301987396994668\n",
    "Test accuracy: 0.3536821705426357"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ False-Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# сколько вообще людей имеют хорошую точку\n",
    "df_all[(df_all.is_train == 1)].groupby('customer_id')['is_work'].agg('max').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = score(df_valid, ys=['is_home'], return_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cid = 'bf66305d0ec05abb6e6a6358acb8c2a1'\n",
    "cid = df_pred[df_pred.is_home == 0].sample(1)['customer_id'].values[0]\n",
    "\n",
    "df_an = df_all[df_all.customer_id == cid]\n",
    "center_home = df_an[['home_lat', 'home_lon']].drop_duplicates().values\n",
    "center_work = df_an[['work_lat', 'work_lon']].drop_duplicates().values\n",
    "\n",
    "\n",
    "predicted_home = df_pred[df_pred.customer_id == cid][['is_home:add_lat', 'is_home:add_lon']].drop_duplicates().values\n",
    "predicted_work = df_pred[df_pred.customer_id == cid][['is_work:add_lat', 'is_work:add_lon']].drop_duplicates().values\n",
    "\n",
    "points_pos = df_an[df_an.is_pos == 1][['add_lat', 'add_lon']].dropna().values\n",
    "points_atm = df_an[df_an.is_pos == 0][['add_lat', 'add_lon']].dropna().values\n",
    "print(center_home.shape, center_work.shape, points_pos.shape, points_atm.shape)\n",
    "\n",
    "# синие - покупки\n",
    "# красные - банкоматы\n",
    "gmap = gmaps.Map()\n",
    "if len(points_pos) > 0:\n",
    "    gmap.add_layer(gmaps.symbol_layer(points_pos, hover_text='pos', \n",
    "                                      fill_color=\"blue\", stroke_color=\"blue\", scale=3))\n",
    "if len(points_atm) > 0:\n",
    "    gmap.add_layer(gmaps.symbol_layer(points_atm, hover_text='atm',\n",
    "                                      fill_color=\"red\", stroke_color=\"red\",scale=3))\n",
    "\n",
    "if not np.isnan(center_home)[0][0]:\n",
    "    gmap.add_layer(gmaps.marker_layer(center_home, label='home'))\n",
    "if not np.isnan(center_work)[0][0]:\n",
    "    gmap.add_layer(gmaps.marker_layer(center_work, label='work'))\n",
    "\n",
    "gmap.add_layer(gmaps.marker_layer(predicted_home, label='predicted_home'))\n",
    "gmap.add_layer(gmaps.marker_layer(predicted_work, label='predicted_work'))\n",
    "    \n",
    "gmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv('../data/dfpredict1903.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cust_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_test = df_all.loc[df_all['is_train'] == 0, 'customer_id'].unique()\n",
    "# df_test = pd.DataFrame(cust_test, columns = ['customer_id']).merge(df_all, how = 'left')\n",
    "df_test = predict_proba(pd.DataFrame(cust_test, columns = ['customer_id']).merge(df_all, how = 'left'))\n",
    "df_test.rename(columns = {\n",
    "        'customer_id':'_ID_',\n",
    "        'is_home:add_lat': '_HOME_LAT_',\n",
    "        'is_home:add_lon': '_HOME_LON_',\n",
    "        'is_work:add_lat': '_WORK_LAT_',\n",
    "        'is_work:add_lon': '_WORK_LON_'}, inplace = True)\n",
    "df_test = df_test[['_ID_', '_WORK_LAT_', '_WORK_LON_', '_HOME_LAT_', '_HOME_LON_']]\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Формируем submission-файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заполняем пропуски\n",
    "df_ = pd.read_csv('../data/test_set.csv', dtype=dtypes, usecols=['customer_id'])\n",
    "submission = pd.DataFrame(df_['customer_id'].unique(), columns=['_ID_'])\n",
    "\n",
    "submission = submission.merge(df_test, how='left').fillna(0)\n",
    "# Пишем файл submission\n",
    "submission.to_csv('../submissions/base_14_635_331.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
